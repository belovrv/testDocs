Start writing here.
DataFrame provides DSL for selecting arbitrary set of columns.  Column selectors are used in many operations:    df.select { age and name } df.fillNaNs { dfsOf<Double>() }.withZero() df.remove { cols { it.hasNulls() } } df.update { city }.notNull { it.lowercase() } df.gather { numberCols() }.into("key", "value") df.move { name.firstName and name.lastName }.after { city }    Select columns by name:      // by column name df.select { it.name } df.select { name } // by column path df.select { name.firstName } // with a new name df.select { name named "Full Name" } // converted df.select { name.firstName.map { it.lowercase() } } // column arithmetics df.select { 2021 - age } // two columns df.select { name and age } // range of columns df.select { name..age } // all children of ColumnGroup df.select { name.all() } // dfs traversal of children columns df.select { name.dfs() }    // by column name val name by columnGroup() df.select { it[name] } df.select { name } // by column path val firstName by name.column<String>() df.select { firstName } // with a new name df.select { name named "First Name" } // converted df.select { firstName.map { it.lowercase() } } // column arithmetics val age by column<Int>() df.select { 2021 - age } // two columns df.select { name and age } // range of columns df.select { name..age } // all children of ColumnGroup df.select { name.all() } // dfs traversal of children columns df.select { name.dfs() }    // by column name df.select { it["name"] } // by column path df.select { it["name"]["firstName"] } df.select { "name"["firstName"] } // with a new name df.select { "name" named "First Name" } // converted df.select { "name"["firstName"]<String>().map { it.uppercase() } } // column arithmetics df.select { 2021 - "age"() } // two columns df.select { "name" and "age" } // by range of names df.select { "name".."age" } // all children of ColumnGroup df.select { "name".all() } // dfs traversal of children columns df.select { "name".dfs() }     Select columns by column index:    // by index df.select { col(2) } // by several indices df.select { cols(0, 1, 3) } // by range of indices df.select { cols(1..4) }    Other column selectors:    // by condition df.select { cols { it.name.startsWith("year") } } // by type df.select { colsOf<String>() } df.select { stringCols() } // by type with condition df.select { colsOf<String> { !it.hasNulls() } } df.select { stringCols { !it.hasNulls() } } // all top-level columns df.select { all() } // first/last n columns df.select { take(2) } df.select { takeLast(2) } // all except first/last n columns df.select { drop(2) } df.select { dropLast(2) } // dfs traversal of columns df.select { dfs() } // dfs traversal with condition df.select { dfs { it.name.contains(":") } } // columns of given type in dfs traversal df.select { dfsOf<String>() } // all columns except given column set df.select { except { colsOf<String>() } } // union of column sets df.select { take(2) and col(3) }    Modify the set of selected columns:    // first/last n columns in column set df.select { dfs().take(3) } df.select { dfs().takeLast(3) } // all except first/last n columns in column set df.select { dfs().drop(3) } df.select { dfs().dropLast(3) } // filter column set by condition df.select { dfs().filter { it.name.startsWith("year") } } // exclude columns from column set df.select { dfs().except { age } }
DataFrame doesn't implement Iterable interface, but redefines some of extension functions available for Iterable :    df.forEachRow { println(it) } df.take(5) df.drop(2) df.chunked(10)    To convert DataFrame into Iterable / Sequence of rows, columns or cell values use the following functions:    df.columns() // List<DataColumn> df.rows() // Iterable<DataRow> df.values() // Sequence<Any?>
Start writing here.
Start writing here.
Start writing here.
Start writing here.
Start writing here.
Inserts new column at specific position in DataFrame .  Similar to add , but supports column positioning.  insert (columnName) { rowExpression } | (column) .under { parentColumn } | .after { column } | .at(position) rowExpression: DataRow.(DataRow) -> Value  Create new column based on existing columns and insert it into DataFrame :      df.insert("year of birth") { 2021 - age }.after { age }    val year = column<Int>("year of birth") val age by column<Int>() df.insert(year) { 2021 - age }.after { age }    df.insert("year of birth") { 2021 - "age"<Int>() }.after("age")     Insert previously created column:    val score by columnOf(4, 5, 3, 5, 4, 5, 3) df.insert(score).at(2)
Replaces one or several columns with new columns.  replace { columns } .with(newColumns) | .with { columnExpression } columnExpression: DataFrame.(DataColumn) -> DataColumn  See column selectors    df.replace { name }.with { name.firstName } df.replace { stringCols() }.with { it.lowercase() } df.replace { age }.with { 2021 - age named "year" }    Note: replace { columns }.with { columnExpression } is equivalent to convert { columns }.to { columnExpression } . See convert
explode - distributes lists of values or dataframes in given columns vertically, replicating data in other columns  implode - collects column values in given columns into lists or dataframes, grouping by other columns
Splits list-like values in one or several columns and spreads them vertically. Values in other columns are duplicated.  This is reverse operation to implode  Exploded columns will change their types:   List<T> to T  DataFrame to DataRow   Note that exploded FrameColumn will convert into ColumnGroup  Rows with empty lists will be skipped. If you want to keep such rows with null value in exploded columns, set dropEmpty flag to false .      val a by columnOf(1, 2) val b by columnOf(listOf(1, 2), listOf(3, 4)) val df = dataFrameOf(a, b) df.explode { b }    val df = dataFrameOf("a", "b")( 1, listOf(1, 2), 2, listOf(3, 4) ) df.explode("b")     When several columns are exploded in one operation, lists in different columns will be aligned.    val a by columnOf(listOf(1, 2), listOf(3, 4, 5)) val b by columnOf(listOf(1, 2, 3), listOf(4, 5)) val df = dataFrameOf(a, b) df.explode { a and b }    DataColumn<Collection> or FrameColumn can also be exploded:    val col by columnOf(listOf(1, 2), listOf(3, 4)) col.explode()      val col by columnOf( dataFrameOf("a", "b")(1, 2, 3, 4), dataFrameOf("a", "b")(5, 6, 7, 8) ) col.explode()
Returns DataFrame in which given String columns are parsed into other types.  Special case of convert operation.    df.parse()    To parse only particular columns use column selector :    df.parse { age and weight }    parse tries to parse every String column into one of supported types in the following order:   Int  Long  Boolean  BigDecimal  LocalDate  LocalDateTime  LocalTime  URL  Double   Available parser options:   locale: Locale is used to parse numbers  dateTimeFormatter: DateTimeFormatter is used to parse date and time  nulls: List<String> is used to treat particular strings as null value. Default null strings: "null" and "NULL"     df.parse(options = ParserOptions(locale = Locale.CHINA, dateTimeFormatter = DateTimeFormatter.ISO_WEEK_DATE))    You can also set global parser options that will be used by default in read , convert and parse operations:    DataFrame.parser.locale = Locale.FRANCE DataFrame.parser.addDateTimeFormat("dd.MM.uuuu HH:mm:ss")
split column values horizontally or vertically  merge values from several columns into single column
Splits every value in the given columns into several values. Splitted values can be spread horizontally or vertically or remain inside the original column as List  The following types of columns can be splitted by default:   String : split by , and trim  List : split into elements  DataFrame : split into rows   df.split { columns } [.by(delimeters) | .by { splitter } | .match(regex)] // how to split cell value [.default(value)] // how to fill nulls .into(columnNames) [ { columnNamesGenerator } ] | .inward(columnNames) [ { columnNamesGenerator } | .inplace() | .intoRows() | .intoColumns() ] // where to store results splitter = DataRow.(T) -> Iterable<Any> columnNamesGenerator = DataColumn.(columnIndex: Int) -> String  Storage options:   into(col1, col2, ... ) - store splitted values in new top-level columns  inward(col1, col2, ...) - store splitted values in new columns nested inside original column  inplace - store splitted values in original column as List  intoRows - spread splitted values vertically into new rows  intoColumns - split FrameColumn into ColumnGroup storing in every cell a List of original values per every column   columnNamesGenerator is used to generate names for additional columns when the list of explicitly specified columnNames was not long enough. columnIndex starts with 1 for the first additional column name.  Default columnNamesGenerator generates column names splitted1 , splitted2 ...   Reverse operation to merge      df.split { name.firstName }.by { it.chars().toList() }.inplace() df.split { name }.by { it.values() }.into("nameParts") df.split { name.lastName }.by(" ").default("").inward { "word$it" }    val name by columnGroup() val firstName by name.column<String>() val lastName by name.column<String>() df.split { firstName }.by { it.chars().toList() }.inplace() df.split { name }.by { it.values() }.into("nameParts") df.split { lastName }.by(" ").default("").inward { "word$it" }    df.split { "name"["firstName"]<String>() }.by { it.chars().toList() }.inplace() df.split { name }.by { it.values() }.into("nameParts") df.split { "name"["lastName"] }.by(" ").default("").inward { "word$it" }     String columns can also be splitted into group matches of Regex pattern:    merged.split { name } .match("""(.*) \((.*)\)""") .inward("firstName", "lastName")    FrameColumn can be splitted into columns:    val df1 = dataFrameOf("a", "b", "c")( 1, 2, 3, 4, 5, 6 ) val df2 = dataFrameOf("a", "b")( 5, 6, 7, 8, 9, 10 ) val group by columnOf(df1, df2) val id by columnOf("x", "y") val df = dataFrameOf(id, group) df.split { group }.intoColumns()     Returns DataFrame with duplicated rows for every splitted value.  Reverse operation to implode .  Use .intoRows() terminal operation in split configuration to spread splitted values vertically:      df.split { name.firstName }.by { it.chars().toList() }.intoRows() df.split { name }.by { it.values() }.intoRows()    val name by columnGroup() val firstName by name.column<String>() df.split { firstName }.by { it.chars().toList() }.intoRows() df.split { name }.by { it.values() }.intoRows()    df.split { "name"["firstName"]<String>() }.by { it.chars().toList() }.intoRows() df.split { group("name") }.by { it.values() }.intoRows()     Equals to split { column }...inplace().explode { column } . See explode for details.
Merges several columns into a single column.  Reverse operation to split  merge { columns } [.notNull()] .by(delimeter) | .by { merger } [.into(column) | .intoList() ] merger: (DataRow).List<T> -> Any    // Merge two columns into one column "fullName" df.merge { name.firstName and name.lastName }.by(" ").into("fullName")    merger accepts a List of collected values for every row typed by their common type:    df.merge { name.firstName and name.lastName } .by { it[0] + " (" + it[1].uppercase() + ")" } .into("fullName")    When heterogeneous columns are merged, they may need to be cast to valid types in merger :    df.merge { name.firstName and age and isHappy } .by { "${it[0]} aged ${it[1]} is " + (if (it[2] as Boolean) "" else "not ") + "happy" } .into("status")    By default, when no delimeter or merger is specified, values will be merged into the List :    df.merge { numberCols() }.into("data")    Merged column values can also be exported to List :    // Merge data from two columns into List<String> df.merge { name.firstName and name.lastName }.by(",").intoList()
DataColumn represents a column of values. It can store objects of primitive or reference types, or other DataFrames .  See how to create columns    name: String - name of the column, should be unique within containing dataframe  type: KType - type of elements in the column  size: Int - number of elements in the column  values: Iterable<T> - column data  hasNulls: Boolean - flag indicating whether column contains null values    DataColumn instances can be one of three subtypes: ValueColumn , ColumnGroup or FrameColumn   Represents a sequence of values.  It can store values of primitive (integers, strings, decimals etc.) or reference types. Currently, it uses List as underlying data storage.   Container for nested columns. Is used to create column hierarchy.   Special case of ValueColumn that stores other DataFrames as elements.  DataFrames stored in FrameColumn may have different schemas.  FrameColumn may appear after reading from JSON or other hierarchical data structures, or after grouping operations such as groupBy or pivot .    ColumnAccessors are used for typed data access in DataFrame :    val age by column<Int>() // Access fourth cell in the "age" column of dataframe `df`. // This expression returns `Int` because variable `age` has `ColumnAccessor<Int>` type. // If dataframe `df` has no column "age" or column "age" has type which is incompatible with `Int`, // runtime exception will be thrown. df[age][3] + 5 // Access first cell in the "age" column of dataframe `df`. df[0][age] * 2 // Returns new dataframe sorted by age column (ascending) df.sortBy(age) // Returns new dataframe with the column "year of birth" added df.add("year of birth") { 2021 - age } // Returns new dataframe containing only rows with age > 30 df.filter { age > 30 }    See how to create column accessor  ColumnAccessor stores column name (for top-level columns) or column path (for nested columns), has type argument that corresponds to column type , but doesn't contain any data. To convert ColumnAccessor into DataColumn just add values:    val age by column<Int>() val ageCol1 = age.withValues(15, 20) val ageCol2 = age.withValues(1..10)
DataRow represents a record, one piece of data within a DataFrame   DataRow properties:   index: Int - sequential row number in DataFrame , starts from 0  prev: DataRow? - previous row ( null for the first row)  next: DataRow? - next row ( null for the last row)   If some of these properties clash with generated extension properties, they still can be accessed as functions index() , prev() , next()  DataRow functions:   getRow(Int): DataRow - row from DataFrame by row index  neighbours(Iterable<Int>) - sequence of the nearest rows by relative index: neighbours(-1..1) will return previous, current and next row  get(column) - cell value from column in this row  values() - list of all cell values from the current row  df() - DataFrame that current row belongs to    Row expressions provide a value for every row of DataFrame and are used in add , filter , forEach , update and other operations:  df.add("fullName") { firstName + " " + lastName }  Row properties are also accessible within row expressions:  df.add("diff") { value - prev?.value } df.filter { index % 5 == 0 }  Row expression signature is DataRow.(DataRow) -> T , so row values can be accessed with or without it keyword. Implicit and explicit argument represent the same DataRow object.   Row condition is a row expression that returns Boolean . Its signature is DataRow.(DataRow) -> Boolean  Row conditions are used in various filtration operations:  df.filter { it.name.startsWith("A") } df.filter { name.length == 5 }  Row properties can be also used in row conditions:  df.filter { index % 2 == 0} df.drop { age == prev?.age } df.update { score }.where { index > 20}.with { prev?.score }
DataFrame transformation pipeline usually consists of several modification operations, such as filtering, sorting, grouping, pivoting, adding/removing columns etc. DataFrame API is designed in functional style so that the whole processing pipeline can be represented as a single statement with a sequential chain of operations. DataFrame object is immutable, so all operations defined for DataFrame object return a new DataFrame instance that reusing underlying data structures as much as possible.   Simple operations (such as filter or select ) return DataFrame , but more complex operations return an intermediate object that is used for further configuration of the operation. Let's call such operations multiplex .    df.update { age }.where { city == "Paris" }.with { it - 5 } .move { name.firstName and name.lastName }.after { isHappy } .merge { age and weight }.by { "Age: ${it[0]}, weight: ${it[1]}" }.into("info") .rename { isHappy }.into("isOkay")    First call in configuration chain for multiplex operation selects a subset of columns in DataFrame for which that operation should be applied. To select columns you can use String API or Column Selectors .  Further operation configuration API may vary depending on types of selected columns.  Multiplex operations usually end with call to into or with function. The following naming convention is used:   into defines column names where operation results should be stored. Used in move , group , split , merge , gather , groupBy , rename .  with defines row-wise data transformation using row expression . Used in update , convert , replace , pivot .     add - add columns  append - add rows  columns - get list of columns  concat - union rows  convert - change column values and/or column types  describe - basic column statistics  distinct / distinctBy - remove duplicated rows  drop / dropLast / dropNulls / dropNa - remove rows be condition  explode - spread list-like values vertically  fillNulls / fillNaNs / fillNA - replace missing values  filter / filterBy - filter rows  first / firstOrNull - first row by condition  flatten - remove column groupings recursively  forEachRow / forEachColumn - iterate over rows or columns  format - conditional formatting for cell rendering  gather - convert columns into key-value pairs  getColumn / getColumnOrNull / getColumnGroup / getColumns - get one or several columns  group - group columns into ColumnGroup  groupBy - group rows by key columns  head - top 5 rows  implode - collapse column values into lists  insert - insert column  join - join dataframes by key columns  last / lastOrNull - last row by condition  map - map DataFrame columns to a new DataFrame or DataColumn  max / maxBy / maxOf / maxFor - max of values  mean / meanOf / meanFor - average of values  median / medianOf / medianFor - median of values  merge - merge several columns into one  min / minBy / minOf / minFor - min of values  move - move columns or change column groupings  ncol - number of columns  parse - convert String values into appropriate types  pivot - convert column values into new columns  remove - remove columns  rename - rename columns  replace - replace columns  rows / rowsReversed  schema - schema of column hierarchy  select - select subset of columns  shuffled - reorder rows randomly  single / singleOrNull - single row by condition  sortBy / sortByDesc / sortWith - sort rows  split - split column values into several columns or new rows  std / stdOf / stdFor - standard deviation of values  sum / sumOf / sumFor - sum of values  take / takeLast - first/last rows  ungroup - remove column grouping  update - change column values preserving column types
Different ways to create dataframes from the data that already loaded into memory, are described in this section. You usually either create Column s from iterables , and then convert them into DataFrame , or create small dataframes for tests using vararg variants of the corresponding functions directly from values.  Creating dataframes from files and URLs is described in the next section
Returns DataFrame with union of columns from several given DataFrames .    df.add(df1, df2)    See all use cases of 'add' operation .
Returns DataFrame with the union of rows from several given DataFrames .    df.concat(df1, df2)      listOf(df1, df2).concat()    See all use cases of 'concat' operation .
Joins two DataFrames by join columns.  join(otherDf, type = JoinType.Inner) [ { joinColumns } ] joinColumns: JoinDsl.(LeftDataFrame) -> Columns interface JoinDsl: LeftDataFrame { val right: RightDataFrame fun DataColumn.match(rightColumn: DataColumn) }  joinColumns is a column selector that defines column mapping for join:      df.join(other) { name match right.fullName }    val name by columnGroup() val fullName by columnGroup() df.join(other) { name match fullName }    df.join(other) { "name" match "fullName" }     If mapped columns have the same name, just select join columns from the left DataFrame :      df.join(other) { name and city }    val name by columnGroup() val city by column<String>() df.join(other) { name and city }    df.join(other, "name", "city")     If joinColumns is not specified, columns with the same name from both DataFrames will be used as join columns:    df.join(other)     Supported join types:   Inner (default) - only matched rows from left and right dataframes  Left - all rows from left dataframe, mismatches from right dataframe filled with null  Right - all rows from right dataframe, mismatches from left dataframe filled with null  Full - all rows from left and right dataframes, any mismatches filled with null  Exclude - only mismatched rows from left   For every join type there is a shortcut operation:      df.innerJoin(other) { name and city } df.leftJoin(other) { name and city } df.rightJoin(other) { name and city } df.fullJoin(other) { name and city } df.excludeJoin(other) { name and city }    val name by columnGroup() val city by column<String>() df.innerJoin(other) { name and city } df.leftJoin(other) { name and city } df.rightJoin(other) { name and city } df.fullJoin(other) { name and city } df.excludeJoin(other) { name and city }    df.innerJoin(other, "name", "city") df.leftJoin(other, "name", "city") df.rightJoin(other, "name", "city") df.fullJoin(other, "name", "city") df.excludeJoin(other, "name", "city")
Start writing here.
Computes the standard deviation of values.  Is available for numeric columns. Computed value has type Double .    df.std() // std of values per every numeric column df.std { age and weight } // std of all values in `age` and `weight` df.stdFor { age and weight } // std of values per `age` and `weight` separately, skips NA df.stdOf { (weight ?: 0) / age } // std of expression evaluated for every row      df.std() df.age.std() df.groupBy { city }.std() df.pivot { city }.std() df.pivot { city }.groupBy { name.lastName }.std()    See statistics for details on complex data aggregations.
To compute one or several statistics per every group of GroupBy use aggregate function. Its body will be executed for every data group and has a receiver of type DataFrame that represents current data group being aggregated. To add new column to the resulting DataFrame , pass the name of new column to infix function into :      df.groupBy { city }.aggregate { nrow() into "total" count { age > 18 } into "adults" median { age } into "median age" min { age } into "min age" maxBy { age }.name into "oldest" }    val city by column<String?>() val age by column<Int>() val name by columnGroup() df.groupBy { city }.aggregate { nrow() into "total" count { age() > 18 } into "adults" median { age } into "median age" min { age } into "min age" maxBy { age() }[name] into "name of oldest" } // or df.groupBy(city).aggregate { nrow() into "total" count { age > 18 } into "adults" median(age) into "median age" min(age) into "min age" maxBy(age)[name] into "name of oldest" }    df.groupBy("city").aggregate { nrow() into "total" count { "age"<Int>() > 18 } into "adults" median("age") into "median age" min("age") into "min age" maxBy("age")["name"] into "oldest" }     If only one aggregation function is used, column name can be omitted:      df.groupBy { city }.aggregate { maxBy { age }.name }    val city by column<String?>() val age by column<Int>() val name by columnGroup() df.groupBy { city }.aggregate { maxBy { age() }[name] } // or df.groupBy(city).aggregate { maxBy(age)[name] }    df.groupBy("city").aggregate { maxBy("age")["name"] }     Most common aggregation functions can be computed directly at GroupBy :      df.groupBy { city }.max() // max for every comparable column df.groupBy { city }.mean() // mean for every numeric column df.groupBy { city }.max { age } // max age into column "age" df.groupBy { city }.sum("total weight") { weight } // sum of weights into column "total weight" df.groupBy { city }.count() // number of rows into column "count" df.groupBy { city } .max { name.firstName.length() and name.lastName.length() } // maximum length of firstName or lastName into column "max" df.groupBy { city } .medianFor { age and weight } // median age into column "age", median weight into column "weight" df.groupBy { city } .minFor { (age into "min age") and (weight into "min weight") } // min age into column "min age", min weight into column "min weight" df.groupBy { city }.meanOf("mean ratio") { weight?.div(age) } // mean of weight/age into column "mean ratio"    val city by column<String?>() val age by column<Int>() val weight by column<Int?>() val name by columnGroup() val firstName by name.column<String>() val lastName by name.column<String>() df.groupBy { city }.max() // max for every comparable column df.groupBy { city }.mean() // mean for every numeric column df.groupBy { city }.max { age } // max age into column "age" df.groupBy { city }.sum("total weight") { weight } // sum of weights into column "total weight" df.groupBy { city }.count() // number of rows into column "count" df.groupBy { city } .max { firstName.length() and lastName.length() } // maximum length of firstName or lastName into column "max" df.groupBy { city } .medianFor { age and weight } // median age into column "age", median weight into column "weight" df.groupBy { city } .minFor { (age into "min age") and (weight into "min weight") } // min age into column "min age", min weight into column "min weight" df.groupBy { city }.meanOf("mean ratio") { weight()?.div(age()) } // mean of weight/age into column "mean ratio"    df.groupBy("city").max() // max for every comparable column df.groupBy("city").mean() // mean for every numeric column df.groupBy("city").max("age") // max age into column "age" df.groupBy("city").sum("weight", name = "total weight") // sum of weights into column "total weight" df.groupBy("city").count() // number of rows into column "count" df.groupBy("city").max { "name"["firstName"].strings().length() and "name"["lastName"].strings().length() } // maximum length of firstName or lastName into column "max" df.groupBy("city") .medianFor("age", "weight") // median age into column "age", median weight into column "weight" df.groupBy("city") .minFor { ("age".ints() into "min age") and ("weight".intOrNulls() into "min weight") } // min age into column "min age", min weight into column "min weight" df.groupBy("city").meanOf("mean ratio") { "weight".intOrNull()?.div("age".int()) } // mean of weight/age into column "mean ratio"     To get all column values for every group without aggregation use values function:   for ValueColumn of type T it will gather group values into lists of type Many<T>  for ColumnGroup it will gather group values into DataFrame and convert ColumnGroup into FrameColumn       df.groupBy { city }.values() df.groupBy { city }.values { name and age } df.groupBy { city }.values { weight into "weights" }    val city by column<String?>() val age by column<Int>() val weight by column<Int?>() val name by columnGroup() df.groupBy(city).values() df.groupBy(city).values(name, age) df.groupBy(city).values { weight into "weights" }    df.groupBy("city").values() df.groupBy("city").values("name", "age") df.groupBy("city").values { "weight" into "weights" }
To aggregate data groups in Pivot or PivotGroupBy with one or several statistics use aggregate :      df.pivot { city }.aggregate { minBy { age }.name } df.pivot { city }.groupBy { name.firstName }.aggregate { meanFor { age and weight } into "means" stdFor { age and weight } into "stds" maxByOrNull { weight }?.name?.lastName into "biggest" }    val city by column<String?>() val name by columnGroup() val firstName by name.column<String>() val age by column<Int>() val weight by column<Int?>() df.pivot { city }.aggregate { minBy(age).name } df.pivot { city }.groupBy { firstName }.aggregate { meanFor { age and weight } into "means" stdFor { age and weight } into "stds" maxByOrNull(weight)?.name?.lastName into "biggest" }    df.pivot("city").aggregate { minBy("age")["name"] } df.pivot("city").groupBy { "name"["firstName"] }.aggregate { meanFor("age", "weight") into "means" stdFor("age", "weight") into "stds" maxByOrNull("weight")?.getColumnGroup("name")?.get("lastName") into "biggest" }     Shortcuts for common aggregation functions are also available:      df.pivot { city }.maxFor { age and weight } df.groupBy { name }.pivot { city }.median { age }    val city by column<String?>() val name by columnGroup() val age by column<Int>() val weight by column<Int?>() df.pivot { city }.maxFor { age and weight } df.groupBy { name }.pivot { city }.median { age }    df.pivot("city").maxFor("age", "weight") df.groupBy("name").pivot("city").median("age")     By default, when aggregation function produces several values for data group, column hierarchy in resulting DataFrame will be indexed first by pivot keys and then by the names of aggregated values. To reverse this order so that resulting columns will be indexed first by names of aggregated values and then by pivot keys, use separate=true flag that is available in multi-result aggregation operations, such as aggregate or <stat>For :      df.pivot { city }.maxFor(separate = true) { age and weight } df.pivot { city }.aggregate(separate = true) { min { age } into "min age" maxOrNull { weight } into "max weight" }    val city by column<String?>() val age by column<Int>() val weight by column<Int?>() df.pivot { city }.maxFor(separate = true) { age and weight } df.pivot { city }.aggregate(separate = true) { min { age } into "min age" maxOrNull { weight } into "max weight" }    df.pivot("city").maxFor("age", "weight", separate = true) df.pivot("city").aggregate(separate = true) { min("age") into "min age" maxOrNull("weight") into "max weight" }     By default, any aggregation function will result in null value for those matrix cells, where intersection of column and row keys produced an empty data group. You can specify default value for any aggregation by default infix function. This value will replace all null results of aggregation function over non-empty data groups as well. To use one default value for all aggregation functions, use default() before aggregation.      df.pivot { city }.groupBy { name }.aggregate { min { age } default 0 } df.pivot { city }.groupBy { name }.aggregate { median { age } into "median age" default 0 minOrNull { weight } into "min weight" default 100 } df.pivot { city }.groupBy { name }.default(0).min()    val city by column<String?>() val age by column<Int>() val weight by column<Int?>() val name by columnGroup() df.pivot { city }.groupBy { name }.aggregate { min { age } default 0 } df.pivot { city }.groupBy { name }.aggregate { median { age } into "median age" default 0 minOrNull { weight } into "min weight" default 100 } df.pivot { city }.groupBy { name }.default(0).min()    df.pivot("city").groupBy("name").aggregate { min("age") default 0 } df.pivot("city").groupBy("name").aggregate { median("age") into "median age" default 0 minOrNull("weight") into "min weight" default 100 } df.pivot("city").groupBy("name").default(0).min()      pivot operation can also be used inside aggregate body of GroupBy . This allows to combine column pivoting with other aggregation functions:      df.groupBy { name.firstName }.aggregate { pivot { city }.aggregate(separate = true) { mean { age } into "mean age" count() into "count" } count() into "total" }    val city by column<String?>() val name by columnGroup() val firstName by name.column<String>() val age by column<Int>() df.groupBy { firstName }.aggregate { pivot { city }.aggregate(separate = true) { mean { age } into "mean age" count() into "count" } count() into "total" }    df.groupBy { "name"["firstName"] }.aggregate { pivot("city").aggregate(separate = true) { mean("age") into "mean age" count() into "count" } count() into "total" }
add - union of columns from several dataframes  concat - union of rows from several dataframes  join - sql-like join of two dataframes by key columns
Start writing here.
Start writing here.
Iterate over rows:      for (row in df) { println(row.age) } df.forEachRow { println(it.age) } df.rows().forEach { println(it.age) }    val age by column<Int>() for (row in df) { println(row[age]) } df.forEachRow { println(it[age]) } df.rows().forEach { println(it[age]) }    for (row in df) { println(row["age"]) } df.forEachRow { println(it["age"]) } df.rows().forEach { println(it["age"]) }     Iterate over columns:    df.forEachColumn { println(it.name()) } df.columns().forEach { println(it.name()) }    Iterate over cells:    // from top to bottom, then from left to right df.values().forEach { println(it) } // from left to right, then from top to bottom df.values(byRow = true).forEach { println(it) }
Get single DataRow by index :    df[2]    Get single DataRow by row condition :      df.single { age == 45 } df.first { weight != null } df.minBy { age } df.maxBy { name.firstName.length } df.maxByOrNull { weight }    val age by column<Int>() val weight by column<Int?>() val name by columnGroup() val firstName by name.column<String>() df.single { age() == 45 } df.first { weight() != null } df.minBy(age) df.maxBy { firstName().length } df.maxByOrNull { weight() }    df.single { "age"<Int>() == 45 } df.first { it["weight"] != null } df.minBy("weight") df.maxBy { "name"["firstName"]<String>().length } df.maxByOrNull("weight")
Return rows of DataFrame as Iterable<DataRow> .   Return rows of DataFrame in reversed order.
Returns the single row that matches the given condition , or throws exception if there is no or more than one matching row.   Returns the single row that matches the given condition , or null if there is no or more than one matching row.
Start writing here.
DataFrame is a data structure that stores data in columns. Columns can store values, other columns or DataFrames, therefore DataFrame can represent any hierarchical data structure.
Start writing here.
Start writing here.
Computes the minimum / maximum of values.  Is available for Comparable columns. null and NaN values are ignored.    df.min() // min of values per every comparable column df.min { age and weight } // min of all values in `age` and `weight` df.minFor { age and weight } // min of values per `age` and `weight` separately df.minOf { (weight ?: 0) / age } // min of expression evaluated for every row df.minBy { age } // DataRow with minimal `age`      df.min() df.age.min() df.groupBy { city }.min() df.pivot { city }.min() df.pivot { city }.groupBy { name.lastName }.min()    See statistics for details on complex data aggregations.
Computes the sum of values.  Is available for numeric columns. null and NaN values are ignored.    df.sum() // sum of values per every numeric column df.sum { age and weight } // sum of all values in `age` and `weight` df.sumFor { age and weight } // sum of values per `age` and `weight` separately df.sumOf { (weight ?: 0) / age } // sum of expression evaluated for every row      df.age.sum() df.groupBy { city }.sum() df.pivot { city }.sum() df.pivot { city }.groupBy { name.lastName }.sum()    See statistics for details on complex data aggregations.
Computes the median of values.  Is available for Comparable columns. null and NaN values are ignored.    df.median() // median of values per every comparable column df.median { age and weight } // median of all values in `age` and `weight` df.medianFor { age and weight } // median of values per `age` and `weight` separately df.medianOf { (weight ?: 0) / age } // median of expression evaluated for every row      df.median() df.age.median() df.groupBy { city }.median() df.pivot { city }.median() df.pivot { city }.groupBy { name.lastName }.median()    See statistics for details on complex data aggregations.
Computes the mean of values.  Is available for numeric columns. Computed value has type Double . Use skipNA flag to skip null and NaN values.    df.mean() // mean of values per every numeric column df.mean(skipNA = true) { age and weight } // mean of all values in `age` and `weight`, skips NA df.meanFor(skipNA = true) { age and weight } // mean of values per `age` and `weight` separately, skips NA df.meanOf { (weight ?: 0) / age } // median of expression evaluated for every row      df.mean() df.age.mean() df.groupBy { city }.mean() df.pivot { city }.mean() df.pivot { city }.groupBy { name.lastName }.mean()    See statistics for details on complex data aggregations.
Returns the first row that matches the given condition , or throws exception if there is no matching rows.   Returns the first row that matches the given condition , or null if there is no matching rows.
Returns the last row that matches the given condition , or throws exception if there is no matching rows.   Returns the last row that matches the given condition , or null if there is no matching rows.
Returns the first row that has the smallest value in the given column, or throws exception if DataFrame is empty.   Returns the first row that has the smallest value in the given column, or null if DataFrame is empty.
Returns the first row that has the largest value in the given column, or throws exception if DataFrame is empty.   Returns the first row that has the largest value in the given column, or null if DataFrame is empty.
This section describes ways to create DataColumn .   Returns new column with given elements. Column type is deduced from compile-time type of elements, column name is taken from the name of the variable.    // Create ValueColumn with name 'student' and two elements of type String val student by columnOf("Alice", "Bob")    To assign column name explicitly, use named infix function and replace by with = .    val column = columnOf("Alice", "Bob") named "student"    When column elements are columns themselves, it returns ColumnGroup :    val firstName by columnOf("Alice", "Bob") val lastName by columnOf("Cooper", "Marley") // Create ColumnGroup with two nested columns val fullName by columnOf(firstName, lastName)    When column elements are DataFrames it returns FrameColumn :    val df1 = dataFrameOf("name", "age")("Alice", 20, "Bob", 25) val df2 = dataFrameOf("name", "temp")("Mark", 36.6) // Create FrameColumn with two elements of type DataFrame val frames by columnOf(df1, df2)     Converts Iterable of values into column.    listOf("Alice", "Bob").toColumn("name")    To compute column type at runtime by scanning through actual values, set Infer.Type option.  To inspect values only for nullability set Infer.Nulls option.    val values: List<Any?> = listOf(1, 2.5) values.toColumn("data") // type: Any? values.toColumn("data", Infer.Type) // type: Number values.toColumn("data", Infer.Nulls) // type: Any     Converts Iterable of values into column of given type    val values: List<Any?> = listOf(1, 2.5) values.toColumnOf<Number?>("data") // type: Number?
This section describes ways to create DataFrame .   Returns DataFrame with given column names and values.    // DataFrame with 2 columns and 3 rows val df = dataFrameOf("name", "age")( "Alice", 15, "Bob", 20, "Mark", 100 )      val name by columnOf("Alice", "Bob", "Mark") val age by columnOf(15, 20, 22) // DataFrame with 2 columns val df = dataFrameOf(name, age)      val names = listOf("name", "age") val values = listOf( "Alice", 15, "Bob", 20, "Mark", 22 ) val df = dataFrameOf(names, values)      // Multiplication table dataFrameOf(1..10) { x -> (1..10).map { x * it } }      // DataFrame with 5 columns filled with 7 random double values: val names = (1..5).map { "column$it" } val df = dataFrameOf(names).randomDouble(7)      val names = listOf("first", "second", "third") // DataFrame with 3 columns, fill each column with 15 `true` values val df = dataFrameOf(names).fill(15, true)     DataFrame from Iterable<DataColumn> :    val name by columnOf("Alice", "Bob", "Mark") val age by columnOf(15, 20, 22) listOf(name, age).toDataFrame()    DataFrame from Map<String, List<*>> :    val map = mapOf("name" to listOf("Alice", "Bob", "Mark"), "age" to listOf(15, 20, 22)) // DataFrame with 2 columns map.toDataFrame()     Creates DataFrame from Iterable of any objects .    data class Person(val name: String, val age: Int) val persons = listOf(Person("Alice", 15), Person("Bob", 20), Person("Mark", 22)) val df = persons.createDataFrame()    Scans object properties using reflection and creates ValueColumn for every property. Scope of properties for scanning is defined at compile-time by formal types of objects in Iterable , so properties of implementation classes will not be scanned.  Specify depth parameter to perform deep object graph traversal and convert nested objects into ColumnGroups and FrameColumns :    data class Name(val firstName: String, val lastName: String) data class Score(val subject: String, val value: Int) data class Student(val name: Name, val age: Int, val scores: List<Score>) val students = listOf( Student(Name("Alice", "Cooper"), 15, listOf(Score("math", 4), Score("biology", 3))), Student(Name("Bob", "Marley"), 20, listOf(Score("music", 5))) ) val df = students.createDataFrame(depth = 2)    For detailed control over object graph transformation use configuration DSL. It allows you to exclude particular properties or classes from object graph traversal, compute additional columns and configure column grouping.    val df = students.createDataFrame { // add value column "year of birth" from { 2021 - it.age } // scan all properties properties(depth = 2) { exclude(Score::subject) // `subject` property will be skipped from object graph traversal preserve<Name>() // `Name` objects will be stored as-is without transformation into DataFrame } // add column group "summary" { "max score" from { it.scores.maxOf { it.value } } "min score" from { it.scores.minOf { it.value } } } }
Column accessors are created by property delegate  column . Column type should be passed as type argument, column name will be taken from the variable name.    val name by column<String>()    To assign column name explicitly, pass it as an argument.    val accessor by column<String>("complex column name")    You can also create column accessors for ColumnGroups and FrameColumns    val columns by columnGroup() val frames by frameColumn()     Deep column accessor references nested columns inside ColumnGroups .    val name by columnGroup() val firstName by name.column<String>()     Computed column accessor evaluates custom expression on every data access.      val fullName by df.column { name.firstName + " " + name.lastName } df[fullName]    val name by columnGroup() val firstName by name.column<String>() val lastName by name.column<String>() val fullName by column { firstName() + " " + lastName() } df[fullName]    val fullName by column { "name"["firstName"]<String>() + " " + "name"["lastName"]<String>() } df[fullName]     When expression depends only on one column, use map :    val age by column<Int>() val year by age.map { 2021 - it } df.filter { year > 2000 }
Working with data, you have to read it - from disk or from remote URLs - and write it on disk. This section describes how to do it. For now, only CSV, TSV and JSON formats are supported.
Group columns into ColumnsGroups .  It is a special case of move operation.  group { columns } .into(groupName) | .into { groupNameExpression } groupNameExpression = DataColumn.(DataColumn) -> String    df.group { age and city }.into("info") df.group { all() }.into { it.type().toString() }.print()    To ungroup grouped columns use ungroup operation.
Replaces ColumnGroup with its nested columns.  Reverse operation to group  ungroup { columns }  See column selectors    // name.firstName -> firstName // name.lastName -> lastName df.ungroup { name }
Returns DataFrame without column groupings under selected columns  flatten [ { columns } ]    // name.firstName -> firstName // name.lastName -> lastName df.flatten { name }    Potential column name clashes are resolved by adding minimal required prefix from ancestor column names.  To remove all column groupings in DataFrame , invoke flatten without parameters:    df.flatten()
insert - inserts new column into DataFrame  replace - replaces columns in DataFrame
DataFrame supports CSV and JSON input formats.  read method automatically detects input format based on file extension and content  DataFrame.read("input.csv")  Input string can be a file path or URL.   All these calls are valid:  import java.io.File import java.net.URL DataFrame.readCSV("input.csv") DataFrame.readCSV(File("input.csv")) DataFrame.readCSV(URL("https://raw.githubusercontent.com/Kotlin/dataframe/master/data/securities.csv"))  All readCSV overloads support different options. For example, you can specify custom delimiter if it differs from , , charset and headers names if your CSV is missing them    val df = DataFrame.readCSV( file, delimiter = '|', headers = listOf("A", "B", "C", "D"), parserOptions = ParserOptions(nulls = setOf("not assigned")) )    Column types will be inferred from the actual CSV data. Suppose that CSV from the previous example had the following content:   A B C D  12 tuv 0.12 true  41 xyz 3.6 not assigned  89 abc 7.1 false   Dataframe schema we get is:  A: Int B: String C: Double D: Boolean?   Basics for reading JSONs are the same: you can read from file or from remote URL.  DataFrame.readJson("https://covid.ourworldindata.org/data/owid-covid-data.json")  Note that after reading a JSON with a complex structure, you can get hierarchical dataframe: dataframe with GroupColumn s and FrameColumn s.  Also note that type inferring process for JSON is much simpler than for CSV. JSON string literals are always supposed to have String type, number literals take different Number kinds, boolean literals are converted to Boolean .  Let's take a look at the following JSON:  [ { "A": "1", "B": 1, "C": 1.0, "D": true }, { "A": "2", "B": 2, "C": 1.1, "D": null }, { "A": "3", "B": 3, "C": 1, "D": false }, { "A": "4", "B": 4, "C": 1.3, "D": true } ]  We can read it from file    val df = DataFrame.readJson(file)    Corresponding dataframe schema will be  A: String B: Int C: Number D: Boolean?  Column A has String type because all values are string literals, no implicit conversion is performed. Column C has Number type because it's the least common type for Int and Double .
DataFrames can be saved in CSV or JSON formats.   You can write your dataframe in CSV format to file, to string or to Appendable (i.e. to Writer ).    df.writeCSV(file)      val csvStr = df.writeCSVStr(CSVFormat.DEFAULT.withDelimiter(';').withRecordSeparator(System.lineSeparator()))     You can write your dataframe in JSON format to file, to string or to Appendable (i.e. to Writer ).    df.writeJson(file)      val jsonStr = df.writeJsonStr(prettyPrint = true)
General information about DataFrame :   nrow() - number of rows  ncol() - number of columns  columnNames() - list of column names  head(n) - first n rows (default 5)  schema() - schema of columns  describe() - general statistics for every column
Returns number of top-level columns in DataFrame .
add columns to DataFrame  map columns to new DataFrame or DataColumn  remove columns from DataFrame
Returns DataFrame which contains all columns from original DataFrame followed by newly added columns. Original DataFrame is not modified.  Create new column and add it to DataFrame :  add(columnName) { rowExpression } rowExpression: DataRow.(DataRow) -> Value      df.add("year of birth") { 2021 - age }    val age by column<Int>() val yearOfBirth by column<Int>("year of birth") df.add(yearOfBirth) { 2021 - age }    df.add("year of birth") { 2021 - "age"<Int>() }     See row expressions  Create and add several columns to DataFrame :  add { columnMapping columnMapping ... } columnMapping = column into columnName | columnName from column | columnName from { rowExpression }      df.add { "year of birth" from 2021 - age age gt 18 into "is adult" name.lastName.length() into "last name length" "full name" from { name.firstName + " " + name.lastName } }    val yob = column<Int>("year of birth") val lastNameLength = column<Int>("last name length") val age by column<Int>() val isAdult = column<Boolean>("is adult") val fullName = column<String>("full name") val name by columnGroup() val firstName by name.column<String>() val lastName by name.column<String>() df.add { yob from 2021 - age age gt 18 into isAdult lastName.length() into lastNameLength fullName from { firstName() + " " + lastName() } }    df.add { "year of birth" from 2021 - "age"<Int>() "age"<Int>() gt 18 into "is adult" "name"["lastName"]<String>().length() into "last name length" "full name" from { "name"["firstName"]<String>() + " " + "name"["lastName"]<String>() } }     Add existing column to DataFrame :    val score by columnOf(4, 3, 5, 2, 1, 3, 5) df.add(score) df + score    Add all columns from another DataFrame :    df.add(df1, df2)
Creates DataFrame or DataColumn with values computed from rows of original DataFrame .  Map into DataColumn :  map(columnName) { rowExpression }: DataColumn rowExpression: DataRow.(DataRow) -> Value  See row expressions  Map into DataFrame :  map { columnMapping columnMapping ... } : DataFrame columnMapping = column into columnName | columnName from column | columnName from { rowExpression } | +column      df.map { "year of birth" from 2021 - age age gt 18 into "is adult" name.lastName.length() into "last name length" "full name" from { name.firstName + " " + name.lastName } +city }    val yob = column<Int>("year of birth") val lastNameLength = column<Int>("last name length") val age by column<Int>() val isAdult = column<Boolean>("is adult") val fullName = column<String>("full name") val name by columnGroup() val firstName by name.column<String>() val lastName by name.column<String>() val city by column<String?>() df.map { yob from 2021 - age age gt 18 into isAdult lastName.length() into lastNameLength fullName from { firstName() + " " + lastName() } +city }    df.map { "year of birth" from 2021 - "age"<Int>() "age"<Int>() gt 18 into "is adult" "name"["lastName"]<String>().length() into "last name length" "full name" from { "name"["firstName"]<String>() + " " + "name"["lastName"]<String>() } +"city" }
Returns DataFrame without selected columns.  remove { columns }  See Column Selectors      df.remove { name and weight }    val name by columnGroup() val weight by column<Int?>() df.remove { name and weight }    df.remove("name", "weight")
Both update and convert can be used to change columns values in DataFrame .  Difference between these operations:   convert allows to change the type of the column, update doesn't  update allows to filter cells to be updated, convert doesn't
Returns DataFrame with changed values in some cells. Column types can not be changed.  update { columns } [.where { rowCondition } ] [.at(rowIndices) ] .with { rowExpression } | .notNull { rowExpression } | .perCol { colExpression } | .perRowCol { rowColExpression } | .withValue(value) | .withNull() | .withZero() rowCondition: DataRow.(OldValue) -> Boolean rowExpression: DataRow.(OldValue) -> NewValue colExpression: DataColumn.(DataColumn) -> NewValue rowColExpression: DataRow.(DataColumn) -> NewValue  See column selectors and row expressions    df.update { age }.with { it * 2 } df.update { dfsOf<String>() }.with { it.uppercase() } df.update { weight }.at(1..4).notNull { it / 2 } df.update { name.lastName and age }.at(1, 3, 4).withNull()    Update with constant value:    df.update { city }.where { name.firstName == "Alice" }.withValue("Paris")    Update with value depending on row:    df.update { city }.with { name.firstName + " from " + it }    Update with value depending on column:    df.update { numberCols() }.perCol { mean(skipNA = true) }    Update with value depending on row and column:    df.update { stringCols() }.perRowCol { row, col -> col.name() + ": " + row.index() }
Replace missing values.   Replaces null values with given value or expression.    df.fillNulls { intCols() }.with { -1 } // same as df.update { intCols() }.where { it == null }.with { -1 }     Replaces Double.NaN and Float.NaN values with given value or expression.    df.fillNaNs { doubleCols() }.withZero()     Replaces null , Double.NaN and Float.NaN values with given value or expression.    df.fillNA { weight }.withValue(-1)
Returns DataFrame with changed values in some columns. Allows to change column types.  convert { columnsSelector } .with { rowExpression } | .perRowCol { rowColExpression } | .withValue(value) | to<Type>() | to { colExpression } rowExpression = DataRow.(OldValue) -> NewValue rowColExpression = DataRow.(DataColumn) -> NewValue colExpression = DataFrame.(DataColumn) -> DataColumn  See column selectors and row expressions    df.convert { age }.with { it.toDouble() } df.convert { dfsOf<String>() }.with { it.toCharArray().toList() }    convert supports automatic type conversions between the following types:   Int  String  Double  Long  Short  Float  BigDecimal  LocalDateTime  LocalDate  LocalTime     df.convert { age }.to<Double>() df.convert { numberCols() }.to<String>() df.convert { name.firstName and name.lastName }.to { it.length() } df.convert { weight }.toFloat()
Returns DataFrame with randomly reordered rows.    df.shuffle()
groupBy - groups rows of DataFrame by given key columns into GroupBy object  concat - concatenates rows from several DataFrames into single DataFrame
Splits the rows of DataFrame into groups using one or several columns as grouping keys.  groupBy { columns } [transformations] [aggregations]  See column selectors , groupBy transformations and groupBy aggregations      df.groupBy { name } df.groupBy { city and name.lastName } df.groupBy { age / 10 named "ageDecade" } df.groupBy { expr { name.firstName.length + name.lastName.length } named "nameLength" }    val name by columnGroup() val lastName by name.column<String>() val firstName by name.column<String>() val age by column<Int>() val city by column<String?>() df.groupBy { name } // or df.groupBy(name) df.groupBy { city and lastName } // or df.groupBy(city, lastName) df.groupBy { age / 10 named "ageDecade" } df.groupBy { expr { firstName().length + lastName().length } named "nameLength" }    df.groupBy("name") df.groupBy { "city" and "name"["lastName"] } df.groupBy { "age".ints() / 10 named "ageDecade" } df.groupBy { expr { "name"["firstName"]<String>().length + "name"["lastName"]<String>().length } named "nameLength" }     Returns GroupBy object.   GroupBy is a DataFrame with one chosen FrameColumn containing data groups.  It supports the following operations:   add  sortBy  map  pivot   Any DataFrame with FrameColumn can be reinterpreted as GroupBy :    val key by columnOf(1, 2) // create int column with name "key" val data by columnOf(df[0..3], df[4..6]) // create frame column with name "data" val df = dataFrameOf(key, data) // create dataframe with two columns df.asGroupBy { data } // convert dataframe to GroupBy by interpreting 'data' column as groups    And any GroupBy can be reinterpreted as DataFrame with FrameColumn :    df.groupBy { city }.toDataFrame()    Use concat to union all data groups of GroupBy into original DataFrame preserving new order of rows produced by grouping:    df.groupBy { name }.concat()    To compute one or several aggregation statistics over GroupBy see GroupBy aggregation
Returns DataFrame with the union of rows from several given DataFrames .  concat is available for:  DataFrame :    df.concat(df1, df2)    Iterable<DataFrame> :    listOf(df1, df2).concat()    Iterable<DataRow> :    val rows = listOf(df[2], df[4], df[5]) rows.concat()    GroupBy :    df.groupBy { name }.concat()    FrameColumn :    val frameColumn by columnOf(df[0..1], df[4..5]) frameColumn.concat()    If you want to union columns (not rows) from several DataFrames , see add .   If input DataFrames have different schemas, every column in resulting DataFrame will have the most common type of the original columns with the same name.  For example, if one DataFrame has column A: Int and other DataFrame has column A: Double , resulting DataFrame will have column A: Number .  Missing columns in dataframes will be filled with null .
Returns number of rows in DataFrame .
Returns list of names for top-level columns of DataFrame .
Returns DataFrame containing first n (default 5) rows.    df.head(10)    Similar to take .
Returns DataFrameSchema object with DataFrame schema description. It can be printed to see column structure.  ColumnGroups are marked by indentation:    df.schema()    Output:  name: firstName: String lastName: String age: Int city: String? weight: Int? isHappy: Boolean  FrameColumns are marked with * :    df.groupBy { city }.schema()    Output:  city: String? group: * name: firstName: String lastName: String age: Int city: String? weight: Int? isHappy: Boolean
Start writing here.
Start writing here.
Start writing here.
Start writing here.
Returns DataFrame with general statistics for all ValueColumns .  ColumnGroups and FrameColumns are traversed recursively down to ValueColumns .  Collected statistics:   name - column name  path - path to the column (for hierarchical DataFrame )  type - type of values  count - number of rows  unique - number of unique values  nulls - number of null values  top - the most common not null value  freq - top value frequency  mean - mean value (for numeric columns)  std - standard deviation (for numeric columns)  min - minimal value (for comparable columns)  median - median value (for comparable columns)  max - maximum value (for comparable columns)     df.describe()    To describe only specific columns, pass them as an argument:      df.describe { age and name.all() }    val age by column<Int>() val name by columnGroup() df.describe { age and name.all() }    df.describe { "age" and "name".all() }
Get rows or columns :    df.columns() // List<DataColumn> df.rows() // Iterable<DataRow> df.values() // Sequence<Any?>    Learn how to:   Access data by index  Slice portion of data  Iterate over data  Get single row  Get single column
df.age[1] df[1].age    val age by column<String>() df[age][1] df[1][age]    df["age"][1] df[1]["age"]
Slicing means cutting a portion of DataFrame by continuous range of rows or columns.     by row indices (including boundaries):    df[1..2] df[0..2, 4..5]    See slice rows for other ways to select subset of rows.   by column indices (including boundaries):    df.select { cols(1..3) }    by column names:      df.select { age..weight }    val age by column<Int>() val weight by column<Int?>() df.select { age..weight }    df.select { "age".."weight" }     See Column Selectors for other ways to select subset of columns.
Returns DataFrame with rows at given indices:    df[0, 3, 4]    Returns DataFrame with rows inside given index ranges (including boundary indices):    df[1..2] df[0..2, 4..5]     Returns DataFrame containing first n rows    df.take(5)     Returns DataFrame containing last n rows    df.takeLast(5)     Returns DataFrame containing all rows except first n rows    df.drop(5)     Returns DataFrame containing all rows except last n rows    df.dropLast(5)
Two ways to create DataFrame with a subset of columns:  indexing:      df[df.age, df.weight]    val age by column<Int>() val weight by column<Int?>() df[age, weight]    df["age", "weight"]     See DataFrame indexing  selecting:      df.select { age and weight }    val age by column<Int>() val weight by column<Int?>() df.select { age and weight } df.select(age, weight)    df.select { "age" and "weight" } df.select("age", "weight")     See column selectors
Start writing here.
Returns DataFrame with rows that satisfy row condition      df.filter { age > 18 && name.firstName.startsWith("A") }    val age by column<Int>() val name by columnGroup() val firstName by name.column<String>() df.filter { age() > 18 && firstName().startsWith("A") } // or df.filter { it[age] > 18 && it[firstName].startsWith("A") }    df.filter { "age"<Int>() > 18 && "name"["firstName"]<String>().startsWith("A") }      Returns DataFrame with rows that have value true in given column of type Boolean .      df.filterBy { isHappy }    val isHappy by column<Boolean>() df.filterBy { isHappy }    df.filterBy("isHappy")
Adds one or several rows to DataFrame  df.append ( "Mike", 15, "John", 17, "Bill", 30)
Start writing here.
Available statistics:   count  sum  min/max  mean  median  std   Every statistic can be used in aggregations of:   DataFrame  DataColumn  GroupBy  Pivot  PivotGroupBy     df.mean() df.age.sum() df.groupBy { city }.mean() df.pivot { city }.median() df.pivot { city }.groupBy { name.lastName }.std()    sum , mean , std are available for numeric columns of types Int , Double , Float , BigDecimal , Long , Byte .  min/max , median are available for Comparable columns.  When statistics x is applied to several columns, it can be computed in several modes:   x(): DataRow computes separate value per every suitable column  x { columns }: Value computes single value across all given columns  xFor { columns }: DataRow computes separate value per every given column  xOf { rowExpression }: Value computes single value across results of row expression evaluated for every row   min and max statistics have additional mode by :   minBy { rowExpression }: DataRow finds a row with minimal result of expression     df.sum() // sum of values per every numeric column df.sum { age and weight } // sum of all values in `age` and `weight` df.sumFor { age and weight } // sum of values per `age` and `weight` separately df.sumOf { (weight ?: 0) / age } // sum of expression evaluated for every row     When statistics is applied to GroupBy , it is computed for every data group.  If statistic is applied in a mode that returns a single value for every data group, it will be stored in a single column named by statistic name.    df.groupBy { city }.mean { age } // [`city`, `mean`] df.groupBy { city }.meanOf { age / 2 } // [`city`, `mean`]    You can also pass custom name for aggregated column:    df.groupBy { city }.mean("mean age") { age } // [`city`, `mean age`] df.groupBy { city }.meanOf("custom") { age / 2 } // [`city`, `custom`]    If statistic is applied in a mode that returns separate value per every column in data group, aggregated values will be stored in columns with original column names.    df.groupBy { city }.meanFor { age and weight } // [`city`, `age`, `weight`] df.groupBy { city }.mean() // [`city`, `age`, `weight`, ...]     When statistics is applied to Pivot or PivotGroupBy , it is computed for every data group.  If statistic is applied in a mode that returns a single value for every data group, it will be stored in matrix cell without any name.    df.groupBy { city }.pivot { name.lastName }.mean { age } df.groupBy { city }.pivot { name.lastName }.meanOf { age / 2 }    If statistic is applied in such a way that it returns separate value per every column in data group, every cell in matrix will contain DataRow with values for every aggregated column.    df.groupBy { city }.pivot { name.lastName }.meanFor { age and weight } df.groupBy { city }.pivot { name.lastName }.mean()    To group columns in aggregation results not by pivoted values, but by aggregated columns, apply separate flag:    df.groupBy { city }.pivot { name.lastName }.meanFor(separate = true) { age and weight } df.groupBy { city }.pivot { name.lastName }.mean(separate = true)
Counts the number of rows.    df.count()    Pass row condition to count number of rows that satisfy to that condition:    df.count { age > 15 }    When count is used in groupBy or pivot aggregations, it counts rows for every data group:    df.groupBy { city }.count() df.pivot { city }.count { age > 18 } df.pivot { name.firstName }.groupBy { name.lastName }.count()
Returns DataFrame where values in given columns are merged into lists grouped by other columns.  This is reverse operation to explode  Imploded columns will change their types:   T to Many<T>  DataRow to DataFrame   Note that imploded ColumnGroup will convert into FrameColumn    df.implode { name and age and weight and isHappy }    Set dropNulls flag to filter
pivot - transforms column values into new columns (long to wide)  gather - collects values from several columns into two key and value columns (wide to long)
Splits the rows of DataFrame and groups them horizontally into new columns based on values from one or several columns of original DataFrame .  Pass a column to pivot function to use its values as grouping keys and names for new columns.      df.pivot { city }    val city by column<String?>() df.pivot { city }    df.pivot("city")     Returns Pivot : an intermediate object that can be configured for further transformation and aggregation of data.  See pivot aggregations  By default, pivoted column will be replaced with new columns generated from its values. Instead, you can nest new columns as sub-columns of original column using inward flag:      df.pivot(inward = true) { city }    val city by column<String?>() df.pivot(inward = true) { city }    df.pivot("city", inward = true)     To pivot several columns in one operation you can combine them using and or then infix function:   and will pivot columns independently  then will create column hierarchy based on possible combinations of column values       df.pivot { city and name.firstName } df.pivot { city then name.firstName }    val city by column<String?>() val name by columnGroup() val firstName by name.column<String>() df.pivot { city and firstName } df.pivot { city then firstName }    df.pivot { "city" and "name"["firstName"] } df.pivot { "city" then "name"["firstName"] }      To create matrix table that is expanded both horizontally and vertically, apply groupBy function at Pivot passing the columns for vertical grouping. Reversed order of pivot and groupBy operations will produce the same result.      df.pivot { city }.groupBy { name } // same as df.groupBy { name }.pivot { city }    val city by column<String?>() val name by columnGroup() df.pivot { city }.groupBy { name } // same as df.groupBy { name }.pivot { city }    df.pivot("city").groupBy("name") // same as df.groupBy("name").pivot("city")     Combination of pivot and groupBy operations returns PivotGroupBy that can be used for further aggregation of data groups within matrix cells.  See pivot aggregations  To group by all columns except pivoted use groupByOther :    df.pivot { city }.groupByOther()    Pivot operation can be performed without any data aggregation:   Pivot object can be converted to DataRow or DataFrame .  GroupedPivot object can be converted to DataFrame .   Generated columns will have type FrameColumn and will contain data groups.    df.pivot { city }.toDataRow() df.pivot { city }.groupBy { name }.toDataFrame()     Pivots with Int count statistics one or several columns preserving all other columns of DataFrame .    df.pivotCount { city } // same as df.pivot(inward = true) { city }.groupByOther().count()     Pivots with Boolean statistics one or several columns preserving all other columns of DataFrame .    df.pivotMatches { city } // same as df.pivot(inward = true) { city }.groupByOther().matches()
Converts several columns into two columns key and value . key column will contain names of original columns, value column will contain values from original columns.  This operation is reverse to pivot  gather { columns } [.explodeLists()] [.cast<Type>()] [.notNull()] [.where { valueFilter }] [.mapKeys { keyTransform }] [.mapValues { valueTransform }] .into(keyColumn, valueColumn) | .keysInto(keyColumn) | .valuesInto(valueColumn) valueFilter: (value) -> Boolean keyTransform: (columnName: String) -> K valueTransform: (value) -> R  See column selectors  Configuration options:   explodeLists - gathered values of type List will be exploded into their elements, so where , cast , notNull and mapValues will be applied to list elements instead of lists themselves  cast - inform compiler about the expected type of gathered elements. This type will be passed to where and mapKeys lambdas  notNull - skip gathered null values  where - filter gathered values  mapKeys - transform gathered column names (keys)  mapValues - transform gathered column values   Storage options:   into(keyColumn, valueColumn) - store gathered key-value pairs in two new columns with names keyColumn and valueColumn  keysInto(keyColumn) - store only gathered keys (column names) in a new column keyColumn  valuesInto(valueColumn) - store only gathered values in a new column valueColumn     pivoted.gather { "London".."Tokyo" }.into("city", "population")      pivoted.gather { "London".."Tokyo" } .cast<Int>() .where { it > 10 } .mapKeys { it.lowercase() } .mapValues { 1.0 / it } .into("city", "density")
Start writing here.
You can use DataFrame in different environments - as any other JVM library. The following sections will show how to use DataFrame in Jupyter , Datalore and in a Gradle project .   You can use DataFrame in Jupyter Notebook and in Jupyter Lab. To start, install the latest version of Kotlin kernel and start your favorite Jupyter client from the command line, for example:  jupyter notebook  In the notebook you only have to write single line to start using dataframe:  %use dataframe  In this case the version which is bundled with the kernel, will be used. If you want to always use the latest version, add another magic before %use dataframe :  %useLatestDescriptors %use dataframe  If you want to use specific version of DataFrame, you can specify it in brackets:  %use dataframe(0.9)  After loading, all essential types will be already imported, so you can start using DataFrame. Enjoy!   To start with DataFrame in Datalore, create a Kotlin notebook first:    As the Notebook you've created is actually a Jupyter notebook, you can follow the instructions in the previous section to turn DataFrame on. The simplest way of doing this is shown on screenshot:     DataFrame is published to Maven Central, so you can simply add the following line to your Kotlin DSL buildscript to depend on it:  dependencies { implementation("org.jetbrains.kotlinx:dataframe:<version>") }  In Groovy DSL buildscript setup is very similar:  dependencies { implementation 'org.jetbrains.kotlinx:dataframe:<version>' }   We provide a Gradle plugin that generates interfaces by your data. To use it in your project, pick up the latest version from here and declare plugin dependency in the plugins block:  plugins { id "org.jetbrains.kotlin.plugin.dataframe" version "<version>" }  Note that it's better to use the same version for a library and plugin to avoid unpredictable errors.   If you are using Maven, Ivy or Bazel to configure your build, you can still use DataFrame in your project. Just follow the instructions for your build system on this page .
DataColumn is a named, typed and ordered collection of elements  DataFrame consists of one or several DataColumns with unique names and equal size  DataRow is a single row of DataFrame and provides a single value for every DataColumn
DataFrame represents a list of DataColumn .  Columns in dataframe must have equal size and unique names.  Learn how to:   Create dataframe  Read dataframe  Get an overview of dataframe  Access data in dataframe  Modify data in dataframe  Aggregate data in dataframe  Combine several dataframes
move - moves given columns or changes column grouping in DataFrame  rename - renames given columns in DataFrame
Moves one or several columns within DataFrame .  move { columns } .into { pathSelector } | .under { parentColumn } | .after { column } | .to(position) | .toTop() | .toLeft() | .toRight() pathSelector: DataFrame.(DataColumn) -> ColumnPath  See Column Selectors  Can be used to change columns hierarchy by providing ColumnPath for every moved column    df.move { age }.toLeft() df.move { weight }.to(1) // age -> info.age // weight -> info.weight df.move { age and weight }.into { pathOf("info", it.name) } df.move { age and weight }.into { "info"[it.name] } df.move { age and weight }.under("info") // name.firstName -> fullName.first // name.lastName -> fullName.last df.move { name.firstName and name.lastName }.into { pathOf("fullName", it.name.dropLast(4)) } // a|b|c -> a.b.c // a|d|e -> a.d.e dataFrameOf("a|b|c", "a|d|e")(0, 0) .move { all() }.into { it.name.split("|").toPath() } // name.firstName -> firstName // name.lastName -> lastName df.move { name.cols() }.toTop() // a.b.e -> be // c.d.e -> de df.move { dfs { it.name == "e" } }.toTop { it.parent!!.name + it.name }    Special cases of move :   group - groups columns into ColumnGroups  ungroup - ungroups ColumnGroups  flatten - removes all column groupings
Renames one or several columns without changing its location in DataFrame  df.rename { columns }.into(name) df.rename { columns }.into { nameExpression } nameExpression = (DataColumn) -> String
group - groups given columns into ColumnGroups .  ungroup - ungroups given ColumnGroups by replacing them with their children columns  flatten - recursively removes all column groupings under given ColumnGroups , remaining only ValueColumns and FrameColumns   These operations are special cases of general move operation.
Get single column by column name:      df.age df.name.lastName    val age by column<Int>() val name by columnGroup() val lastName by name.column<String>() df[age] df[lastName]    df["age"] df["name"]["firstName"]     Get single column by index (starting from 0):    df.getColumn(2) df.getColumnGroup(0).getColumn(1)
Return top-level columns of DataFrame as List<DataColumn<*>>
Return column by column name or column selector as DataColumn . Throws exception if requested column doesn't exist.      df.getColumn { age }    val age by column<Int>() df.getColumn { age }    df.getColumn("age")      Return top-level column by column name or column selector as DataColumn or null if requested column doesn't exist.      df.getColumnOrNull { age }    val age by column<Int>() df.getColumnOrNull(age)    df.getColumnOrNull("age")      Return top-level column by column name or column selector as ColumnGroup . Throws exception if requested column doesn't exist or is not a ColumnGroup .      df.getColumnGroup { name }    val name by columnGroup() df.getColumnGroup(name)    df.getColumnGroup("name")      Return list of selected columns.      df.getColumns { age and name }    val age by column<Int>() val name by columnGroup() df.getColumns { age and name }    df.getColumns("age", "name")
DataFrame object is immutable and all operations return a new instance of DataFrame .  Learn how to:   Slice rows  Filter rows  Reorder rows  Select columns  Update/convert values  Split/merge values  Group rows by keys  Append values  Add/remove columns  Move/rename columns  Insert/replace columns  Explode/implode columns  Pivot/gather columns  Map DataFrame
Removes all rows that satisfy row condition      df.drop { weight == null || city == null }    val name by columnGroup() val weight by column<Int?>() val city by column<String?>() df.drop { weight() == null || city() == null } // or df.drop { it[weight] == null || it[city] == null }    df.drop { it["weight"] == null || it["city"] == null }      Remove rows with null values    df.dropNulls() // remove rows with null value in any column df.dropNulls(whereAllNull = true) // remove rows with null values in all columns df.dropNulls { city } // remove rows with null value in 'city' column df.dropNulls { city and weight } // remove rows with null value in 'city' OR 'weight' columns df.dropNulls(whereAllNull = true) { city and weight } // remove rows with null value in 'city' AND 'weight' columns     Remove rows with null , Double.NaN or Float.NaN values    df.dropNA() // remove rows containing null or Double.NaN in any column df.dropNA(whereAllNA = true) // remove rows with null or Double.NaN in all columns df.dropNA { weight } // remove rows where 'weight' is null or Double.NaN df.dropNA { age and weight } // remove rows where either 'age' or 'weight' is null or Double.NaN df.dropNA(whereAllNA = true) { age and weight } // remove rows where both 'age' and 'weight' are null or Double.NaN
Removes duplicate rows. The rows in the resulting DataFrame are in the same order as they were in the original DataFrame .    df.distinct()    If columns are specified, resulting DataFrame will have only given columns with distinct values.      df.distinct { age and name } // same as df.select { age and name }.distinct()    val age by column<Int>() val name by columnGroup() df.distinct { age and name } // same as df.select { age and name }.distinct()    df.distinct("age", "name") // same as df.select("age", "name").distinct()      Keep only the first row for every group of rows grouped by some condition.      df.distinctBy { age and name } // same as df.groupBy { age and name }.mapToRows { group.first() }    val age by column<Int>() val name by columnGroup() val firstName by name.column<String>() df.distinctBy { age and name } // same as df.groupBy { age and name }.mapToRows { group.first() }    df.distinctBy("age", "name") // same as df.groupBy("age", "name").mapToRows { group.first() }
Start writing here.
Returns DataFrame sorted by one or several columns.  By default, columns are sorted in ascending order with null values going first. Available modifiers:   .desc - changes column sort order from ascending to descending  .nullsLast - forces null values to be placed at the end of the order       df.sortBy { age } df.sortBy { age and name.firstName.desc } df.sortBy { weight.nullsLast }    val age by column<Int>() val weight by column<Int?>() val name by columnGroup() val firstName by name.column<String>() df.sortBy { age } df.sortBy { age and firstName } df.sortBy { weight.nullsLast }    df.sortBy("age") df.sortBy { "age" and "name"["firstName"].desc } df.sortBy { "weight".nullsLast }      Returns DataFrame sorted by one or several columns in descending order.      df.sortByDesc { age and weight }    val age by column<Int>() val weight by column<Int?>() df.sortByDesc { age and weight }    df.sortByDesc("age", "weight")      Returns DataFrame sorted with comparator.    df.sortWith { row1, row2 -> when { row1.age < row2.age -> -1 row1.age > row2.age -> 1 else -> row1.name.firstName.compareTo(row2.name.firstName) } }